# Multiple training results

**************************************************
Vocab size: 10,000
loss start: 7.1679
loss end: 3.7282
batch rate: 7820/88394 for first epoch
heads number: 64
layers number: 12
embedding size: 1024
block size: 100
spent minutes: I did not note
learning rate start from: 0.0007
**************************************************
Vocab size: 10,000
loss start: 7.1834
loss end:  3.7681
batch rate: 7820/88394 for first epoch
heads number: 16
layers number: 12
embedding size: 1024
block size: 100
spent minutes: 761
learning rate start from: 0.0007
**************************************************
Vocab size: 14,000
loss start: 7.8421
loss end:  4.4238
batch rate: 9340/70535 for first epoch
heads number: 16
layers number: 12
embedding size: 1024
block size: 100
spent minutes: 1262
learning rate start from: 0.0007
**************************************************
Vocab size: 14,000
loss start: 7.9787
loss end:  4.6414
batch rate: 5590/70535 for first epoch
heads number: 16
layers number: 8
embedding size: 1024
block size: 100
spent minutes: 706
learning rate start from: 0.0007
**************************************************
Vocab size: 10,000
loss start: 7.1766
loss end:  4.2839
batch rate: 2880/88394 for first epoch
heads number: 16
layers number: 8
embedding size: 1024
block size: 100
spent minutes: 180
learning rate start from: 0.0007
**************************************************
Vocab size: 10,000
loss start: 7.2205
loss end: 3.5548
batch rate: 12630/88394 for first epoch
heads number: 12
layers number: 6
embedding size: 768
block size: 100
spent minutes: 637
learning rate start from: 0.0007
**************************************************
Vocab size: 10,000
loss start: I did not note
loss end: 2.9901, It increase the loss after it achieve 2.97
batch rate: 66290/8839 for second epoch
heads number: 8
layers number: 4
embedding size: 256
block size: 100
spent minutes: 1412
learning rate start from: 0.0007
**************************************************

The previous one is fine tunned by increasing layers to 6
Vocab size: 10,000
loss end: 2.8955
batch rate: 29580/88394 for first epoch in the new training
heads number: 8
layers number: 6
embedding size: 256
block size: 100
spent minutes: 591
learning rate start from: 0.0007
**************************************************
Vocab size: 10,000
loss start: 8.2499
batch size: 24
loss end: 2.9
batch rate: 
heads number: 10
layers number: 8
embedding size: 320
block size: 120
spent minutes: 
learning rate start from: 0.0004
I did not note some results but it could not ge below 2.9 after about 1000 minutes
**********************
Vocab size: 10,000
loss start: 8.2 or 8.1
batch size: 64
loss end:  4.0590
batch rate: 1680/4796 epoch 2
heads number: 10
layers number: 6
embedding size: 320
block size: 256
spent minutes: 2069
learning rate start from: 0.0001
dropout_p = 0.15
last gap: 1.1178
**********************
